{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearna regresija\n",
    "\n",
    "**Motivacija**: Izučavanje od jednostavnog algoritma da shvatimo suštinu koncepta i kako zaista radi u pozadini će nam dati osnove za dalje proučavanje drugih algoritama za nadgledano učenje.\n",
    "\n",
    "**Cilj**: Razumijeti jednostavnu linearnu regresiju, šta radi ona, kako možemo računati kvalitet modela linearne regresije, koje metrike postoje za mjerenje grešaka u postavkama linearne regresije, kako određujemo koeficijente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jednostavna linearna regresija\n",
    "\n",
    "Jednostavni pristup nadgledanog učenja, koristan za predviđanje kvantitativnog odgovora $Y$ na osnovnu jednog prediktora $X$. Pretpostavlja se da imamo aproksimativno linearni odnos između $X$ i $Y$. Ovaj odnos možemo napisati kao:\n",
    "\n",
    "$$ Y \\approx \\beta_0 + \\beta_1X $$\n",
    "\n",
    "gdje $\\beta_0$ i $\\beta_1$ su dvije nepoznate konstante koje predstavljaju odječak i i nagib pravca u linearnom modelu. Zajedno, $\\beta_0$ i $\\beta_1$ se zovu još i koeficijenti ili parametri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jednostavna linearna regresija\n",
    "\n",
    "Nakon što smo iskoristili trening skup da procijenimo $\\hat{\\beta}_0$ i $\\hat{\\beta}_1$ za koeficijente modela, možemo predvidjeti buduće kvantitativne odgovore računajući\n",
    "\n",
    "$$ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x $$\n",
    "\n",
    "gdje $\\hat{y}$ pokazuje predikciju $Y$ na osnovu $X = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje koeficijenata\n",
    "\n",
    "Neka nam je dat skup\n",
    "\n",
    "$$ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) $$\n",
    "\n",
    "koji predstavljaju $n$ parova posmatranja, gdje svaki posjeduju mjeru $X$ i mjeru $Y$. Naš cilj je dobiti koeficijente $\\hat{\\beta}_0$ i $\\hat{\\beta}_1$ tako da linearni model *fit*-a dostupne podatke dobro, to jeste, tako da $y_i \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1x_i$ za $i = 1, 2, \\dots, n$. Želimo da nađena linija je što bliže moguća ovim $n$ posmatranjima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje koeficijenata\n",
    "\n",
    "Najčešća mjera za blizinu modela jeste kriterij _najmanjih kvadrata_.\n",
    "\n",
    "Neka $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i$ bude predikcija za $Y$ na osnovu $i$-te vrijednosti $X$. Onda $e_i = y_i - \\hat{y}_i$ predstavlja $i$-ti *rezidual* - razlika između $i$-te vrijednosti posmatranog odgovora i $i$-te vrijednosti predikcije odgovora od našeg modela. Definišemo RSS (*rezidualna suma kvadrata*)\n",
    "\n",
    "$$ \\text{RSS} = e_1^2 + e_2^2 + \\dots + e_n^2 $$\n",
    "\n",
    "ili, ekvivalentno\n",
    "\n",
    "$$ \\text{RSS} = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1x_2)^2 + \\dots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1x_n)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje koeficijenata\n",
    "\n",
    "Najmanji kvadrati koriste $\\hat{\\beta}_0$ i $\\hat{\\beta}_1$ kako bi minimizirali koeficijente. Iz RSS, parcijalnom derivacijom RSS po $\\hat{\\beta}_0$ ćemo dobiti minimajzer za $\\hat{\\beta}_0$, a za minimajzer za $\\hat{\\beta}_1$ slično uradimo parcijalno derivacijom po $\\hat{\\beta}_1$ dobijamo:\n",
    "\n",
    "$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} $$\n",
    "\n",
    "$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} $$\n",
    "\n",
    "gdje $\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^{n}y_i$ i $\\bar{x} \\equiv \\frac{1}{n}\\sum_{i=1}^{n}x_i$ su aritmetičke sredine uzoraka (*sample means*). Drugim riječima, ovi minimajzeri definišu *koeficijente procijenjenim najmanjim kvadratima* za jednostavnu linearnu regresiju. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Sjetimo se da pretpostavljamo da *pravi* odnos između $X$ i $Y$ ima formu $Y = f(X) + \\epsilon$ za neku nepoznatu funkciju $f$, gdje $\\epsilon$ je nul-aritmetičke sredine slučajni član greške. Ukoliko $f$ će biti aproksimirana linearnom funkcijom, onda možemo napisati ovaj odnos kao:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1X + \\epsilon $$\n",
    "\n",
    "$\\beta_0$ predstavlja član odsječka, to jeste, očekivana vrijednost $Y$ kada je $X = 0$, a $\\beta_1$ predstavlja nagib, što znači prosječni rast $Y$ povezana sa rastom jedinice mjere u $X$, dok član greške obuhvata ostale tačke koje jednostavni model promaši. Pretpostavljamo da član greške ne zavisi od $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X + \\epsilon$ predstavlja *populacijsku regresionu liniju*, što je najbolja linearna aproksimacija pravog odnosa između $X$ i $Y$. Regresioni koeficijenti najmanjih kvadrata karakteriziraju *liniju najmanjih kvadrata*. \n",
    "\n",
    "Nažalost, iako imamo metodu koja nam pronalazi procjenu za $\\beta_0$ i $\\beta_1$, međutim, procjena zavisi od skupa posmatranja, gdje prilikom ažuriranja se mijenja linija najmanjih kvadrata, dok se populacijska regresiona linija ne mijenja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Ako bismo procijenili $\\beta_0$ i $\\beta_1$ na osnovu datog skupa podataka, onda naše procjene neće biti iste vrijednosti kao $\\beta_0$ i $\\beta_1$. Međutim, ako bismo mogli našli prosjek procjena pronađenim na osnovu ogromnog broja skupa podataka, onda naše procjene bi bile tačne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Analogije radi, pretpostavimo da želimo znati populacijski prosjek $\\mu$ od neke slučajne varijable $Y$. Ovaj $\\mu$ je nepoznat, ali imamo pristup $n$ posmatranja iz $Y$, koje možemo zapisati kao $y_1, y_2, \\dots, y_n$ i koje možemo koristiti za procjenu $\\mu$. Razumljivo je da možemo izračunati prosjek uzorka i dodijeliti procjeni $\\hat{\\mu} = \\bar{y}$ , gdje $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i$. Treba napomenuti da prosjek uzorka i prosjek populacije su različiti.\n",
    "\n",
    "Analogija između linearne regresije i procjene prosjeka slučajne varijable je pogodna na osnovu koncepta *bias*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Iskorištavajući ovu analogiju, pitanje se postavlja koliko je tačna naša prosjek uzorka $\\hat{\\mu}$ kao procjena $\\mu$? Zaključimo da prosjek $\\hat{\\mu}$ preko mnogo skupova podataka će biti vrlo blizu $\\mu$, ali jedna procjena $\\hat{\\mu}$ može biti veliko podcjenjivanje ili precjenjivanje $\\mu$.\n",
    "\n",
    "Koliko daleko bi bila ta jedna procjena $\\hat{\\mu}$? Odgovaramo ovo pitanje izračunavajući *standardnu grešku* od $\\hat{\\mu}$ napisano kao $\\text{SE}(\\hat{\\mu})$:\n",
    "\n",
    "$$ \\text{Var}(\\hat{\\mu}) = \\text{SE}(\\hat{\\mu}) = \\frac{\\sigma^2}{n} $$\n",
    "\n",
    "gdje $\\sigma$ je standardna devijacija od svake realizacije $y_i$ od $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "U sličnoj situaciji za linearnu regresiju, možemo isto pitanje postaviti za $\\hat{\\beta_0}$ i $\\hat{\\beta_1}$ koliko su blizu pravim vrijednostima $\\beta_0$ i $\\beta_1$. Da bismo izračunali standardne greške koje su vezane za $\\hat{\\beta}_0$ i $\\hat{\\beta}_1$, koristimo sljedeće formule:\n",
    "\n",
    "$$ \\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2\\Big[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\Big] $$\n",
    "\n",
    "$$ \\text{SE}(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} $$\n",
    "\n",
    "gdje $\\sigma^2 = \\text{Var}(\\epsilon)$. Da bi ove formule bile striktno validne, moramo pretpostaviti da greške $\\epsilon_i$ za svako posmatranje nisu u korelaciji sa opštom varijansom $\\sigma^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Općenito, $\\sigma^2$ nije poznata, ali se može procijeniti iz datih podataka. Ova procjena je poznata kao *rezidualna standardna greška* i data je formulom\n",
    "\n",
    "$$ \\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n - 2}}$$\n",
    "\n",
    "Standardne greške se mogu iskoristiti za računanje *intervali sigurnosti (confidence interval)*. 95% interval sigurnosti je definisana kao domet vrijednosti tako da sa vjerovatnoćom od 95%, domet će sadržavati pravu nepoznatu vrijednost parametara. Domet je definirana u odnosu na donju i gornju granicu izračunati iz uzorka podataka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Za linearnu regresiju, 95% interval sigurnosti za $\\beta_1$ aproksimativno ima formu\n",
    "\n",
    "$$ \\hat{\\beta}_1 \\pm 2 \\cdot \\text{SE}({\\hat{\\beta}}_0) $$\n",
    "\n",
    "Standardne greške se također koriste za izvršavanje *test hipoteze* na koeficijentima. Najčešća test hipoteza uključuje testiranje *nul hipoteze*:\n",
    "\n",
    "$$H_0: \\text{Ne postoji odnos između } X \\text{ i } Y \\rightarrow H_0: \\beta_1 = 0$$\n",
    "\n",
    "protiv *alternativne hipoteze*:\n",
    "\n",
    "$$H_a: \\text{Postoji neki odnos između } X \\text{ i } Y \\rightarrow H_a: \\beta_1 \\neq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti procjena koeficijenata\n",
    "\n",
    "Da testiramo nul hipotezu, moramo zaključiti da li $\\hat{\\beta}_1$, naša procjena za $\\beta_1$, je dovoljno daleko od 0 tako da možemo biti sigurni da $\\beta_1$ nije 0. Ovo zavisi, naravno, od tačnosti $\\hat{\\beta}_1$, to jeste, zavisi od standardne greške $\\text{SE}(\\hat{\\beta}_1)$. Ako ova greška je mala, onda čak i relativno male vrijednosti $\\hat{\\beta}_1$ mogu pružati jak dokaz da $\\beta_1 \\neq 0$, otuda onda postoji veza između $X$ i $Y$. Suprotno, ako je greška velika, onda $\\hat{\\beta}_1$ mora biti velika u apsolutnoj vrijednosti kako bismo odbili nul hipotezu.\n",
    "\n",
    "Primjenjuje se *t-statistika* za zaključivanje odnosa između $X$ i $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti modela\n",
    "\n",
    "Nakon što smo odbacili nul hipotezu, prirodno je da želimo kvantificirati do kojeg obima model *fit*-uje podatke. Kvalitet modela linearne regresije se tipično procjenjuje koristeći dvije povezane mjere: *rezidualna standardna greška* (RSE) i $R^2$ statistika.\n",
    "\n",
    "Sjetimo se našeg modela $ Y = \\beta_0 + \\beta_1X + \\epsilon $. Zbog prisutnosti ovih članova greške $\\epsilon$, čak i da znamo pravu regresionu liniju, ne bismo mogli perfektno predvijeti $Y$ iz $X$. RSE je procjena standardne devijacije $\\epsilon$. Računa se koristeći formulu:\n",
    "\n",
    "$$ \\text{RSE} = \\sqrt{\\frac{1}{n - 2}\\text{RSS}} = \\sqrt{\\frac{1}{n - 2}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "RSE se smatra kao mjeru nedostatak modela prilikom *fit*-ovanja podataka. Ako su predikcije dobijenim modelom blizu pravim vrijednostima odgovora, to jeste, ako je $\\hat{y}_i \\approx y_i$ za $i = 1, 2, \\dots n$, onda RSE će biti mal, otuda možemo zaključiti da model *fit*-uje podatke vrlo dobro. U drugu ruku, ako su $\\hat{y}_i$ i $y_i$ vrlo udaljeni za jedno ili više posmatranja, onda RSE može biti vrlo velik, pokazujući da model ne *fit*-uje podatke dobro.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti modela\n",
    "\n",
    "RSE nudi apsolutnu mjeru nedostatka modela prilikom *fit*-ovanja podataka. S obzirom da se računa u jedinicama Y, nije uvijek jasno šta čini dobar RSE. $R^2$ statistika nudi alternativnu mjeru *fit*-ovanja. Forme je proporcije - proporcije objašnjene varijance - i uvijek ima vrijednost između 0 i 1, te je nezavisna od skale od Y. \n",
    "\n",
    "Da se izračuna $R^2$, koristimo formulu\n",
    "\n",
    "$$ R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\n",
    "\n",
    "gdje $\\text{TSS} = \\sum(y_i - \\bar{y})^2$ je *totalna suma kvadrata* i $\\text{RSS} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$. TSS mjeri totalnu varijansu u odgovoru Y i može se zamisliti kao količina varijabilnosti naslijeđeno u odgovoru prije nego što se regresije izvršila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti modela\n",
    "\n",
    "U kontrastu, RSS mjeri količinu varijabilnosti koja je ostala neobjašnjena nakon izvršenja regresije. Otuda, TSS - RSS mjeri količinu varijabilnosti u odgovoru koja je objašnjena od izvršenja regresije, a $R^2$ mjeri proporciju varijabilnosti u $Y$ koja se može objasniti koristeći $X$.\n",
    "\n",
    "Vrijednost $R^2$ blizu 1 pokazuje da velika proporcija varijabilnosti u odgovoru je objašnjena putem regresije. Vrijednost blizu 0 pokazuje da regresija nije objasnila dobar dio varijabilnosti u odgovoru. Ovo se može desiti ukoliko je linearni model pogrešan ili je naslijeđena greška $\\sigma^2$ velika ili oboje.\n",
    "\n",
    "$R^2$ statistika ima interpretacijsku prednost nad RSE, jer za razliku od RSE, $R^2$ se uvijek nalazi između 0 i 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procjenjivanje tačnosti modela\n",
    "\n",
    "Međutim, i dalje može biti izazovno šta čini dobru $R^2$ vrijednost i općenito će zavisiti od primjene.\n",
    "\n",
    "$R^2$ statistika je mjera linearne veze između $X$ i $Y$. Sjetimo se korelacije:\n",
    "\n",
    "$$\\text{Cor}(X,Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "je isto linearni odnos između $X$ i $Y$. Ovo nagovještava da mogli bismo uzeti $r = \\text{Cor}(X, Y)$ umjesto $R^2$ kako bismo procijenili *fit*-ovanje linearnog modela. Može se pokazati da u jednostavnoj linearnoj regresiji da je $R^2 = r^2$, ali u drugim postavkama su oni različiti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
